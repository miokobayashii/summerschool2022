{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practice01_ANN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNgqTHn7c5s8KAfxkt6hs7Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["\n","\n","\n","## **Practice 01_1 Programming Artificial Neural Network**\n","\n","practice with small data set"],"metadata":{"id":"c9QSR9wl1nN5"}},{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/miokobayashii/summerschool2022/blob/main/day01/Practice01_ANN.ipynb)"],"metadata":{"id":"7xFxMUdjYuui"}},{"cell_type":"markdown","source":["Build an artificial neural network which has\n"," \n","\n","*   an input layer with 2 nodes, \n","*   a hidden layer with 4 nodes, and \n","*   a single node output layer.\n","\n","As an activation function, we will use \n","*   Leaky ReLU function for the hidden layer, and\n","*   Sigmoid function for an output layer. \n","\n","\\\n","\n","<img src=\"https://docs.google.com/uc?id=1F55coG7rc2RWMBSVgoUy_3haGRUP63R8\" width=\"500\">\n"],"metadata":{"id":"sTrUvC0YtOKe"}},{"cell_type":"markdown","source":["## Build a model"],"metadata":{"id":"PKGdrIXUGFV_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CF_GyHCD1mY4"},"outputs":[],"source":["from keras.layers import Dense\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.models import Sequential\n","\n","from keras.optimizers import adam_v2\n","\n","from tensorflow.keras import initializers\n","from keras.utils.vis_utils import plot_model\n","import numpy as np"]},{"cell_type":"code","source":["model = Sequential()\n","\n","model.add(Dense(4, input_shape=(2,)))\n","\n","# Leaky ReLU activation\n","model.add(LeakyReLU(alpha=0.01))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.build()\n","\n","model.predict([[1,2]])"],"metadata":{"id":"qM6lX_WQ12Jl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()\n","plot_model(model, to_file='model.png', show_shapes=True)"],"metadata":{"id":"lwKYowJyU9kO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Please think about the reason why the number of the total parameters is 17 in this model.\n"],"metadata":{"id":"XW07vVnHGOrV"}},{"cell_type":"markdown","source":["Write down your answer here.\n","\n","\n"],"metadata":{"id":"J0JTOJJOGoMn"}},{"cell_type":"markdown","source":["\n","\n","\n","\n","---\n","\n","\n","## **Practice 01_2 Confirmation of out values of a hidden layer**\n"],"metadata":{"id":"kVoJlZjZgOYj"}},{"cell_type":"markdown","source":["\n","\\\n","\n","<img src=\"https://docs.google.com/uc?id=1pT8BKpKTWTMprMsOGjApvZzEPcp8mNOM\" width=\"460\">"],"metadata":{"id":"MsDh1zPokuQ3"}},{"cell_type":"markdown","source":["### Here, we fix the values of weight and bias of ANN, and then we confirm the values of A-D.\n"],"metadata":{"id":"KXAcUBeMgUOo"}},{"cell_type":"code","source":["# Fully connected layer\n","w_value = [　, , , , , , , ]\n","b1_value = [　,　,　,　]\n","\n","#To fix weight and bias, we use  \"initializers.Constant\"  function.\n","w_initializer= initializers.Constant(value=w_value)\n","b1_initializer= initializers.Constant(value=b1_value)\n","\n","model = Sequential()\n","model.add(Dense(　　, input_shape=(　,), kernel_initializer=w_initializer, bias_initializer=b1_initializer))\n","\n","# Leaky ReLU activation\n","model.add(LeakyReLU(alpha=0.01))\n","\n","model.build()"],"metadata":{"id":"RPYbJ95HFORk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out2 = model.predict([[　　,　　　]])"],"metadata":{"id":"u7TYtZ5rVwEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Print the output values of the hidden layer\n","print(out2.reshape((out2.shape[0],out2.shape[1])))"],"metadata":{"id":"N9o5dFRAVxTP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Your Answer\n"],"metadata":{"id":"f_0DOZHLXeAn"}},{"cell_type":"markdown","source":["### 1. Answer the values of A-D"],"metadata":{"id":"mhXxkbh2XxvF"}},{"cell_type":"markdown","source":["A = \n","\n","B = \n","\n","C = \n","\n","D = \n","\n"],"metadata":{"id":"K2Jd2U0VYWzg"}},{"cell_type":"markdown","source":["### 2. Change alpha of Leaky ReLU function to 0.1, and answer how the result changes, and the reason."],"metadata":{"id":"9MyC-Sh9X94N"}},{"cell_type":"markdown","source":[" Your answer:\n","\n","---\n","\n","(Write down your answer here)\n"],"metadata":{"id":"x8_fP3J4YCcv"}},{"cell_type":"markdown","source":["\n","---\n","\n","\n","## **Practice 01_3 Confirmation of an output value of an output layer**\n"],"metadata":{"id":"B6yFnGaBmGJ-"}},{"cell_type":"markdown","source":["\\\n","\n","<img src=\"https://docs.google.com/uc?id=1zvTyGf_8WMVEt2Gk5jMR3BkQREuhXkCI\" width=\"480\">\n","\n"],"metadata":{"id":"ndYPs7eaqynK"}},{"cell_type":"markdown","source":["### Here, we fix the values of weight and bias of ANN, and then we confirm the values of E."],"metadata":{"id":"UUi_PjVPrU6y"}},{"cell_type":"code","source":["# Fully connected layer\n","w_value = \n","b1_value =\n","\n","#To fix weight and bias, we use  \"initializers.Constant\"  function.\n","w_initializer= initializers.Constant(value=w_value)\n","b1_initializer= initializers.Constant(value=b1_value)\n","\n","model = Sequential()\n","model.add(Dense(4, input_shape=(2,), kernel_initializer=w_initializer, bias_initializer=b1_initializer))\n","\n","# Leaky ReLU activation\n","model.add(LeakyReLU(alpha=0.01))\n","\n","# Output layer with sigmoid activation\n","v_value = \n","b2_value = \n","\n","v_initializer= initializers.Constant(value=v_value)\n","b2_initializer= initializers.Constant(value=b2_value)\n","model.add(Dense(1, activation='sigmoid', kernel_initializer=v_initializer, bias_initializer=b2_initializer))\n","\n","model.build()"],"metadata":{"id":"llSu6TSYlm9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out3 = model.predict([[ , ]])"],"metadata":{"id":"Wa0Cr6szrn0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(out3)"],"metadata":{"id":"kSaGYOMFr0XB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["E = "],"metadata":{"id":"llXJlI0Htskv"}},{"cell_type":"markdown","source":["### Put the sum of A-E into a sigmoid function, then, is the result same to that of the output layer?"],"metadata":{"id":"zGKvk_yLshPU"}},{"cell_type":"code","source":["#out2 is the output values A-E of the hidden layer. Then, the bias of the output layer is now zero.\n","\n","np.sum(out2)"],"metadata":{"id":"Jw5DrfehgE3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#definition of a sigmoid function\n","\n","def sigmoid(x):\n","  return 1.0 / (1.0 + np.exp(-x))"],"metadata":{"id":"to96qhBFf7jh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sigmoid(np.sum(out2))"],"metadata":{"id":"1SKLfGCxgeai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Please write down your consideration and comments here:\n","\n","\n","\n","\n"],"metadata":{"id":"Vyps_zwbtv4b"}}]}