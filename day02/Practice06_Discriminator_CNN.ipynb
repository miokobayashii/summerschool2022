{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice06_Discriminator_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMUph5X5sg9I9OEMn2XEK/N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator | Convolutional Neural Network\n",
        "\n",
        "We use mnist dataset.\n",
        "\n",
        "Assume that images of ONE are REAL(1), and those of SEVEN are FAKE(0).\n",
        "\n",
        "We investigate whether a discriminator can distinguish ONE from SEVEN.\n",
        "\n",
        "<img src=\"https://docs.google.com/uc?id=1XXmgT15DAbcN1tNI8OvefeTdq4cevU40\" width=\"300\"> <br/>\n",
        "\n",
        "<img src=\"https://docs.google.com/uc?id=1gaPSSjsATGxhR87g9iEw4UVNcX0iF9Bk\" width=\"700\">\n"
      ],
      "metadata": {
        "id": "c9QSR9wl1nN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/miokobayashii/summerschool2022/blob/main/day02/Practice06_Discriminator_CNN.ipynb)"
      ],
      "metadata": {
        "id": "4nCYDAKBDxrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF_GyHCD1mY4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.layers import (\n",
        "    Activation, BatchNormalization, Dense, Dropout, Flatten, Reshape, MaxPooling2D)\n",
        "\n",
        "\n",
        "from keras.layers import Dense, Flatten, Reshape\n",
        "from keras.layers.advanced_activations import LeakyReLU \n",
        "from keras.models import Sequential\n",
        "#from keras.optimizers import Adam\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.layers.convolutional import Conv2D\n",
        "#from tensorflow.keras.utils import plot_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 28)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "\n",
        "img_shape = (img_rows, img_cols, channels)    "
      ],
      "metadata": {
        "id": "U10iLo3n1ymd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(img_shape):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(  , kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
        "\n",
        "    model.add(LeakyReLU(alpha=0.01))            \n",
        "\n",
        "    model.add(Conv2D(  , kernel_size=3, strides=2, padding='same'))\n",
        "\n",
        "    #model.add(BatchNormalization())             \n",
        "\n",
        "    model.add(LeakyReLU(alpha=0.01))            \n",
        "\n",
        "    model.add(Conv2D(  , kernel_size=3, strides=2, padding='same'))\n",
        "\n",
        "    #model.add(BatchNormalization())             \n",
        "\n",
        "    model.add(LeakyReLU(alpha=0.01))            \n",
        "\n",
        "    #model.add(MaxPooling2D(pool_size=2, strides=None, padding='valid'))\n",
        "\n",
        "    model.add(Flatten())                        \n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "qM6lX_WQ12Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                      optimizer=adam_v2.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "discriminator.summary()\n",
        "plot_model(discriminator, to_file='model.png', show_shapes=True)\n"
      ],
      "metadata": {
        "id": "xSVDCBRX13GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "MgukTQKD63CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "1Iirt2I_A-MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_img(img, figsize=(2,2)):\n",
        "    fig = plt.figure(figsize=figsize,dpi=100)\n",
        "    plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
        "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tr7orXhMGl3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "char = 1\n",
        "X_test = X_test[np.where(y_test==char)]  \n",
        "\n",
        "show_img(X_test[1])"
      ],
      "metadata": {
        "id": "_G4OSyA2GqJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_test[1])"
      ],
      "metadata": {
        "id": "ygaUDYCAhcSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "def train(batch_size, sample_interval, n_epochs = 1):\n",
        "\n",
        "    # Load the MNIST dataset\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    char = 1 #True image\n",
        "    X_train_1 = X_train[np.where(y_train==char)]    \n",
        "\n",
        "    char = 7 #Fake image\n",
        "    X_train_7 = X_train[np.where(y_train==char)]  \n",
        "\n",
        "    # Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
        "    X_train_1 = X_train_1 / 127.5 - 1.0\n",
        "    X_train_7 = X_train_7 / 127.5 - 1.0\n",
        "   # X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    #\n",
        "    #bat_per_epo = int((X_train_1.shape[0] + X_train_7.shape[0]) / batch_size)\n",
        "    half_batch = int(batch_size / 2)\n",
        "\n",
        "    #print(bat_per_epo, half_batch )\n",
        "    # Labels for real images: all ones\n",
        "    real = np.ones((half_batch, 1))\n",
        "    # Labels for fake images: all zeros\n",
        "    fake = np.zeros((half_batch, 1))\n",
        "    \n",
        "    bat_per_epo = 10\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        for j in range(bat_per_epo):\n",
        "\n",
        "          # -------------------------\n",
        "          #  Train the Discriminator\n",
        "          # -------------------------\n",
        "\n",
        "          # Get a random batch of real images\n",
        "          idx = np.random.randint(0, X_train_1.shape[0], half_batch)\n",
        "          imgs = X_train_1[idx]\n",
        "\n",
        "          # Get a random batch of fake images\n",
        "          idx = np.random.randint(0, X_train_7.shape[0], half_batch)\n",
        "          fake_img = X_train_7[idx]\n",
        "\n",
        "          # Generate a batch of fake imagesã€€(random noize of [-1 1])\n",
        "          #fake_img = np.random.normal(-1, 1, (batch_size, 28, 28))\n",
        "          #fake_img = np.zeros((batch_size, 28, 28)) -1.0\n",
        "          #print(fake_img.shape)\n",
        "\n",
        "          # Train Discriminator\n",
        "          d_loss_real = discriminator.train_on_batch(imgs, real)\n",
        "          d_loss_fake = discriminator.train_on_batch(fake_img , fake)\n",
        "        # print(d_loss_real, d_loss_fake)\n",
        "          d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "          if (i*10 +  j + 1) % sample_interval == 0 :\n",
        "\n",
        "            # Save losses and accuracies so they can be plotted after training\n",
        "            losses.append((d_loss))\n",
        "            accuracies.append(100.0 * accuracy)\n",
        "            iteration_checkpoints.append(i*10 +  j + 1)\n",
        "\n",
        "            # Output training progress\n",
        "            print(\"Epoch:%d %d/%d [D loss: %f, acc.: %.2f%%]\" %\n",
        "                  (i + 1, j+1, bat_per_epo, d_loss, 100.0 * accuracy))\n",
        "    \n",
        " "
      ],
      "metadata": {
        "id": "r_afy2X12E3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.metrics_names"
      ],
      "metadata": {
        "id": "dkHJB90ZDumY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "batch_size = 128\n",
        "sample_interval = 1\n",
        "\n",
        "# Train the Discriminator for the specified number of iterations\n",
        "train(batch_size, sample_interval)"
      ],
      "metadata": {
        "id": "_lfBo1k63k_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "char = 1 #True image: A label for 1 is \"1\"\n",
        "X_test_1 = X_test[np.where(y_test==char)]    \n",
        "y_test_1 = y_test[np.where(y_test==char)]  \n",
        "y_test_1 = np.ones((y_test_1.shape[0], 1))\n",
        "\n",
        "char = 7 #Fake image: A label for 7 is \"0\"\n",
        "X_test_7 = X_test[np.where(y_test==char)]  \n",
        "y_test_7 = y_test[np.where(y_test==char)]  \n",
        "y_test_7 = np.zeros((y_test_7.shape[0], 1))\n",
        "\n"
      ],
      "metadata": {
        "id": "2nfDWA3mU8sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = discriminator.evaluate(X_test_1, y_test_1)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "metadata": {
        "id": "QkY-MuXLVTMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = discriminator.evaluate(X_test_7, y_test_7)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "metadata": {
        "id": "KxlMCiJsV_Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fake_img = np.zeros((batch_size, 28, 28)) -1\n",
        " #fake_img = np.random.normal(-1, 1, (batch_size, 28, 28))\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "char = 1 #True image\n",
        "X_test_1 = X_test[np.where(y_test==char)]    \n",
        "\n",
        "char = 7 #Fake image\n",
        "X_test_7 = X_test[np.where(y_test==char)]  \n",
        "\n",
        "test_size = 10\n",
        "\n",
        "#### test 1: real image\n",
        "idx = np.random.randint(0, X_test_1.shape[0], test_size)\n",
        "imgs_1 = X_test_1[idx]\n",
        "\n",
        "for i in range(test_size):\n",
        "  print(i, end=\" \")\n",
        "  show_img(imgs_1[i])\n",
        "\n",
        "y = discriminator.predict(imgs_1)\n",
        "print(y)\n",
        "\n",
        "false = np.where(y<=0.5)\n",
        "print(false)\n",
        "\n",
        "\n",
        "#### test 7: fake image\n",
        "idx = np.random.randint(0, X_test_7.shape[0], test_size)\n",
        "imgs_7 = X_test_7[idx]\n",
        "\n",
        "for i in range(test_size):\n",
        "  print(i, end=\" \")\n",
        "  show_img(imgs_7[i])\n",
        "\n",
        "y = discriminator.predict(imgs_7)\n",
        "print(y)\n",
        "false = np.where(y>=0.5)\n",
        "print(false)"
      ],
      "metadata": {
        "id": "ITNAjFYu5aeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = np.array(losses)\n",
        "\n",
        "# Plot training losses for Discriminator and Generator\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, losses, label=\"Discriminator loss\")\n",
        "\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "-qcC-pNkFy8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}